{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a157a0-0b7e-4a0d-900c-79bc0fc3d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!glxgears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6dba8-e355-492b-b096-59ff723e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet-18 for classifying roof materials from PlanetScope SuperDove imagery\n",
    "Case study in Washington, D.C. \n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "from torchsat.models.classification import resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "from shapely.geometry import box\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion() # interactive\n",
    "\n",
    "# Projection information\n",
    "wgs = from_epsg(4326)\n",
    "proj = from_epsg(32618)\n",
    "print(f'Projected CRS: {proj}')\n",
    "\n",
    "# maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "maindir = '/home/jovyan/opp-data' # jetstream2\n",
    "\n",
    "print(\"Successfully imported all packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3bf8-31c5-456f-a965-3c7bc461ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoofImageDataset_Planet(Dataset):\n",
    "    \"\"\"Class to handle PlanetScope SuperDove imagery for Resnet-18\"\"\"\n",
    "\n",
    "    def __init__(self, gdf, img_path, n_bands, img_dim, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gdf: Geodataframe containing 'geometry' column and 'class_code' column\n",
    "            img_path: the path to the PlanetScope SuperDove composite image (single mosaic file)\n",
    "                - see 'psscene-prep.py' for spectral indices calculation\n",
    "            imgdim (int): Image dimension for CNN implementation\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "\n",
    "        Returns image chunks with class labels\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise ValueError(f'Image does not exists: {img_path}')\n",
    "\n",
    "        self.geometries = [p.centroid for p in gdf.geometry.values] # gather centroid geoms\n",
    "        self.img_path = img_path # path to image data\n",
    "        self.img_dim = img_dim # resnet window dimension, defaults to 64\n",
    "        self.n_bands = n_bands # number of bands in the input image\n",
    "        self.Y = gdf.code.values # class codes (numeric)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.geometries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the geometry of the idx (centroid)\n",
    "        geom = self.geometries[idx]\n",
    "\n",
    "        try:\n",
    "            sample = self.sample_image(geom)  # run the sampling function\n",
    "        \n",
    "            cc = self.Y[idx]  # get the class codes\n",
    "            if type(cc) != int:\n",
    "                cc = cc.astype('uint8') # make sure the cc is an integer\n",
    "            \n",
    "            # Ensure the sample has the correct dimensions\n",
    "            assert sample.shape == (self.n_bands, self.img_dim, self.img_dim), f'Invalid sample shape: {sample.shape}'\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "            print(f\"Skipping invalid sample at index: {idx}\")\n",
    "            sample = torch.from_numpy(np.zeros((self.n_bands, int(self.img_dim), int(self.img_dim))))\n",
    "            cc = 255 # highest int8 number to be flagged\n",
    "         \n",
    "        # Convert the sample array to a Torch object\n",
    "        sample = torch.from_numpy(sample)\n",
    "\n",
    "        # Return the sample and the label as torch objects\n",
    "        return {'image': sample.type(torch.FloatTensor),\n",
    "                'code': torch.tensor(cc).type(torch.LongTensor)}\n",
    "\n",
    "    \n",
    "    def sample_image(self, geom):\n",
    "        \"\"\" Sample the image at each geometry for the specified image chunk size (window) \"\"\"\n",
    " \n",
    "        N = self.img_dim # window size to be used for cropping\n",
    "            \n",
    "        # Use the windows.from_bounds() method to return the window\n",
    "        # Returns image chunks from training data locations\n",
    "        with rio.open(self.img_path) as src:\n",
    "            py, px = src.index(geom.x, geom.y)\n",
    "            window = rio.windows.Window(px - N // 2, py - N // 2, N, N)\n",
    "            # print(window)\n",
    "            \n",
    "            # Read the data in the window\n",
    "            # clip is a nbands * N * N numpy array\n",
    "            clip = src.read(window=window, indexes=list(range(1, self.n_bands + 1)))\n",
    "\n",
    "            del py, px, window # clean up\n",
    "\n",
    "        # Convert the image chunk to a numpy array\n",
    "        clip_arr = np.array(clip)\n",
    "\n",
    "        # Check if the image chunk has valid data\n",
    "        if clip_arr.sum() > 0:\n",
    "            # Mask invalid values in each band independently\n",
    "            ans = np.ma.masked_equal(clip_arr, 0).filled(0)\n",
    "        else:\n",
    "            ans = clip_arr\n",
    "        \n",
    "        del clip, clip_arr # clean up\n",
    "        return ans\n",
    "\n",
    "\n",
    "def make_good_batch(batch):\n",
    "    \"\"\"\n",
    "    Removes bad samples if image dimensions do not match.\n",
    "    Args:\n",
    "        - batch: list of dictionaries, each containing 'image' tensor and 'code' tensor\n",
    "    returns: list of dictionaries same as input with samples having non-matching image dims removed\n",
    "    \"\"\"\n",
    "\n",
    "    _idx = torch.where(batch['code'] != 255)[0] # good batches\n",
    "\n",
    "    new_batch = {}\n",
    "    new_batch['image'] = batch['image'][_idx]\n",
    "    new_batch['code'] = batch['code'][_idx]\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def balance_sampling(df, ratio=5, strategy='undersample'):\n",
    "    \"\"\"\n",
    "    Generate balanced sample from training data based on the defined ratio.\n",
    "    This can be done with majority undersampling or minority oversampling ('strategy' parameter)\n",
    "    Args:\n",
    "        - df: the dataframe with rows as training data\n",
    "        - ratio: the sampling ration (i.e., 5:1 for minority classes default)\n",
    "    Returns:\n",
    "        - random sample with class ratios as defined\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the class counts\n",
    "    class_counts = df['class_code'].value_counts()\n",
    "    min_class_count = class_counts.min()\n",
    "    \n",
    "    # Calculate the target count for each class based on the ratio\n",
    "    target_count = {\n",
    "        class_label: min(min_class_count * ratio, len(df[df['class_code'] == class_label])) \n",
    "                        for class_label in class_counts.index\n",
    "    }\n",
    "    \n",
    "    # Create an empty list to store balanced dataframes\n",
    "    balanced_dfs = []\n",
    "    for class_label in class_counts.index:\n",
    "        class_df = df[df['class_code'] == class_label]\n",
    "        if strategy == 'undersample':\n",
    "            # Under-sample the majority class\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=False, n_samples=target_count[class_label], random_state=42)\n",
    "        elif strategy == 'oversample':\n",
    "            # Over-sample the minority class\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=True, n_samples=target_count[class_label], random_state=42)\n",
    "        balanced_dfs.append(balanced_class_df)\n",
    "\n",
    "    # Concatenate the results by class\n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def split_training_data(gdf, ts, vs):\n",
    "    \"\"\" \n",
    "    Splits dataframe into train, test, and validation samples with the defined ratios \n",
    "    Args:\n",
    "        - gdf: training samples (geo data frame)\n",
    "        - ts: test size #\n",
    "        - vs: validation size #\n",
    "    Returns:\n",
    "        train, test, and validation dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df, test_df, val_df = [], [], []\n",
    "\n",
    "    for cl in gdf.class_code.unique():\n",
    "        \n",
    "        # subset to class\n",
    "        _gdf = gdf.loc[gdf.class_code == cl]\n",
    "        \n",
    "        # get train and test validation arrays. \n",
    "        # test array is validation array split in half.\n",
    "        _train, _valtest = train_test_split(_gdf, random_state=27, test_size=ts)\n",
    "        train_df.append(_train)\n",
    "        \n",
    "        _val, _test = train_test_split(_valtest, random_state=27, test_size=vs)\n",
    "        test_df.append(_test)\n",
    "        val_df.append(_val)\n",
    "\n",
    "    # Concatenate the samples across classes\n",
    "    all_train_df = pd.concat(train_df)\n",
    "    all_train_df = gpd.GeoDataFrame(all_train_df, crs=gdf.crs)\n",
    "    \n",
    "    all_val_df = pd.concat(val_df)\n",
    "    all_val_df = gpd.GeoDataFrame(all_val_df, crs=gdf.crs)\n",
    "    \n",
    "    all_test_df = pd.concat(test_df)\n",
    "    all_test_df = gpd.GeoDataFrame(all_test_df, crs=gdf.crs)\n",
    "\n",
    "    return all_train_df, all_val_df, all_test_df\n",
    "\n",
    "\n",
    "print(\"Class and functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0a05-5aed-4233-a6f5-c06462bae509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73acf7f-7546-4aa7-880c-489596ef92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan')\n",
    "print(os.getcwd())\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f6824-7a2c-4d67-913e-3714fc33e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308774c2-5428-4247-adb3-836626ae7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (footprints)\n",
    "ref_path = 'opp-data/dc_data_reference_footprints.gpkg'\n",
    "ref = gpd.read_file(ref_path)\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845358-d7eb-4350-89fd-5ae7dbbc2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the class imbalance in the reference data\n",
    "print(f\"Class counts:\\n\\n{ref.description.value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18afdd2-df96-4bc6-8b64-c445134f787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref['code'], _ = pd.factorize(ref['class_code']) # create a factorized version\n",
    "print(ref['class_code'].value_counts())  # check the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030b0e8-3624-4946-8227-09db94678667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping class_code to code\n",
    "code_mapping = dict(zip(ref['class_code'], ref['code']))\n",
    "desc_mapping = dict(zip(ref['class_code'], ref['description']))\n",
    "print(f'Code map: {code_mapping}\\nDescription map: {desc_mapping}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415691cf-bb2e-4ee7-96b3-f2827defbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average footprint area and side length\n",
    "mean_area_sqm = int(ref.areaUTMsqft.values.mean()) * 0.092903\n",
    "pct95_area_sqm = np.percentile(ref.areaUTMsqft, 95) * 0.092903\n",
    "print(f'Mean footprint area (sqm): {mean_area_sqm}')\n",
    "print(f'95th percentile footprint area (sqm): {pct95_area_sqm}')\n",
    "# Calculate the side length\n",
    "pct95_side_length = int(np.sqrt(pct95_area_sqm))\n",
    "print(f'95th percentile side length (m): {pct95_side_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a332a-a3fc-439e-bfe4-1d826f49ab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4744c31-4015-4033-b26b-dedf8d307a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify 'pure' training locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b445b9-c0b0-4c40-bc4e-cfeb3a4e54c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3abce-71b8-4c90-ac78-62976e6da18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3bda0-63d2-41f9-815d-dabf141005e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centroids\n",
    "ref_pt = ref.copy()\n",
    "ref_pt = ref_pt.to_crs(epsg=32618) # UTM Zone 18N\n",
    "ref_pt['geometry'] = ref_pt['geometry'].centroid\n",
    "\n",
    "# Define the window size and half window (for boxes)\n",
    "window_size = 48 # 4 times the average side length\n",
    "half_window = window_size / 2\n",
    "\n",
    "training_windows = [] # image windows with >50% of specific roof type\n",
    "training_roof_types = [] # roof type codes for valid windows\n",
    "\n",
    "# Loop through each footprint individually\n",
    "for geom, roof_type in zip(ref.geometry, ref['class_code']):\n",
    "\n",
    "    centroid = geom.centroid # footprint centroid\n",
    "    \n",
    "    # calculate the image window (64x64)\n",
    "    window = box(centroid.x - half_window, centroid.y - half_window,\n",
    "                 centroid.x + half_window, centroid.y + half_window)\n",
    "\n",
    "    # Intersect with centroids to get class count within window\n",
    "    intersect = ref_pt[ref_pt.intersects(window)]\n",
    "    \n",
    "    # Get the total count and count for the class\n",
    "    total_count = len(intersect)\n",
    "    class_count = len(intersect[intersect['class_code'] == roof_type])\n",
    "\n",
    "    # Check if there is at least 50% of the roof type in that window\n",
    "    if total_count > 0 and (class_count / total_count) > 0.50:\n",
    "        training_windows.append(centroid)\n",
    "        training_roof_types.append(roof_type)\n",
    "\n",
    "    del intersect, window, centroid\n",
    "\n",
    "# Create a GeoDataFrame for the training windows with roof types\n",
    "ref_windows = gpd.GeoDataFrame({\n",
    "    'geometry': training_windows, \n",
    "    'class_code': training_roof_types\n",
    "}, crs=ref.crs)\n",
    "\n",
    "# Create a numeric code for the training data frame\n",
    "ref_windows['code'], _ = pd.factorize(ref_windows['class_code'])\n",
    "print(\"Spatial filtering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98549a0-35a2-42e8-99dc-7670877ba0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out\n",
    "os.getcwd()\n",
    "out_file = 'opp-data/dc_rooftop_materials_training_windows.gpkg'\n",
    "ref_windows.to_file(out_file)\n",
    "\n",
    "del training_windows, training_roof_types, ref_pt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57fbb3-082c-4791-9015-184aafd7e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training locations with colors based on roof type\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ref_windows.plot(column='class_code', ax=ax, legend=True, cmap='Set1', edgecolor='black')\n",
    "plt.title('Training Locations by Roof Material Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21632019-fe26-4043-ba34-66d6cb676242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref_windows.class_code.value_counts())\n",
    "# Create a dictionary mapping class_code to code\n",
    "class_mapping = dict(zip(ref_windows['class_code'], ref_windows['code']))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f55055-37f9-4faa-a915-731b49e56330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform balanced sampling (random undersampling)\n",
    "ref_bal = balance_sampling(ref_windows, ratio=50, strategy='undersample')\n",
    "ref_bal.code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7758d-b7b0-4642-88e1-f70b94f25552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a0e89-b7d6-4fb2-8a39-ee3bfba17681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8adf5-149b-438c-92d6-1afe0ceaa7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df331a-d7e5-401c-8077-f5e2053039c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/test data\n",
    "train_df, val_df, test_df = split_training_data(ref_bal, ts=0.4, vs=0.2)\n",
    "\n",
    "# Print the class distribution in training and validation sets to verify stratification\n",
    "print(\"Train class distribution:\\n\", train_df['code'].value_counts())\n",
    "print(\"Validation class distribution:\\n\", val_df['code'].value_counts())\n",
    "print(\"Test class distribution:\\n\", test_df['code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd122a-1ab3-425c-9390-4aca8b23f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f485d2-4279-499b-8dc9-661390f1acb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c22fa-5f03-49be-b08b-b47275bdffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca50c71-761b-48d5-95ca-e23731769ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join('opp-data/dc_0623_psscene8b_final_norm.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, mask=True, cache=False).squeeze()\n",
    "n_bands = stack_da.values.shape[:1][0]\n",
    "print(\n",
    "    f\"shape: {stack_da.rio.shape}\\n\"\n",
    "    f\"bands: {n_bands}\\n\"\n",
    "    f\"resolution: {stack_da.rio.resolution()}\\n\"\n",
    "    f\"bounds: {stack_da.rio.bounds()}\\n\"\n",
    "    f\"sum: {stack_da.sum().item()}\\n\"\n",
    "    f\"CRS: {stack_da.rio.crs}\\n\"\n",
    "    f\"NoData: {stack_da.rio.nodata}\\n\"\n",
    "    f\"Array: {stack_da}\"\n",
    ")\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225acc4-3619-4a3e-8b58-ec80b7c69f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa6c5a-d8df-40b8-8f71-c6a0dab67eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c39b8-dff5-482f-84cb-3b94c0a145fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464a06e-7e88-49d1-a9d7-8f55e36ee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Resnet-18 model\n",
    "\n",
    "n_bands = n_bands\n",
    "\n",
    "# Define whether to leverage cpu or gpu (for my local machine it is only cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # get device for gpu or cpu\n",
    "print(f'Using {device} for model dev ...')\n",
    "\n",
    "# Grab the number of classes\n",
    "n_classes = ref_bal.class_code.unique().shape[0]\n",
    "print(f'There are {n_classes} roof type classes.')\n",
    "\n",
    "# Define the Resnet-18 model (in_channels = number of bands in the image)\n",
    "model = resnet18(n_classes, in_channels=n_bands, pretrained=False)\n",
    "\n",
    "# Make model parallel and on GPU\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    #ps_model = nn.DataParallel(ps_model)\n",
    "    model = nn.DataParallel(model)\n",
    "    print('Made cpu parallel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33d04c-8d55-4079-9ad3-c1cc4e10df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in each class\n",
    "val_counts = list(train_df['code'].value_counts())\n",
    "print(val_counts)\n",
    "\n",
    "total_samples = sum(val_counts)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = [total_samples / count for count in val_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the calculated class weights for verification\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42775290-3fd6-4b70-9784-ede4e6bb2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa012653-1a9f-4e0c-acb2-b8f1b6a6025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train DataFrame indices: {val_df.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1879f-01f1-439f-b4d0-2be752265dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3258d-1867-4032-bf15-3f6c969aeb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06f20d-bacc-4c95-b992-661464d98416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb4a03-0281-4f37-a103-32a52498ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import make_scorer\n",
    "import itertools\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'window_size': [36, 72, 144],\n",
    "    'batch_size': [64, 128, 224],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Create a parameter grid\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "print(f'There are {len(param_list)} parameter combinations to test!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aafb47-da21-482a-8723-0c5c23e6f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_tuning(params):\n",
    "    ''' Parameter testing for simple Resnet-18 '''\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    window_size = params['window_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Create the training samples\n",
    "    train_ds = RoofImageDataset_Planet(train_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Create the validation samples\n",
    "    val_ds = RoofImageDataset_Planet(val_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    # Initialize lists to track the losses for each epoch\n",
    "    losses = []\n",
    "    epoch_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    val_losses_c = 0.0 # for cumulative val_loss\n",
    "    running_loss = 0.0 # to store epoch loss\n",
    "\n",
    "    epoch_accuracy = []\n",
    "    epoch_precision = []\n",
    "    epoch_recall = []\n",
    "    epoch_f1 = []\n",
    "    epoch_time = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(12):  # Adjust number of epochs as needed\n",
    "        epoch_t0 = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # Model training\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # Ensure a good batch\n",
    "            batch = make_good_batch(batch)\n",
    "            \n",
    "            # Extract samples\n",
    "            image, target = batch['image'].to(device), batch['code'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(image.float()) \n",
    "            \n",
    "            loss = criterion(output, target.long())\n",
    "            running_loss += loss.item() # keep track of the loss\n",
    "            losses.append((idx, loss.item())) # append to list\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            del image, target, batch, output, loss\n",
    "            gc.collect()\n",
    "\n",
    "        # average loss for the epoch\n",
    "        epoch_losses.append(np.array(losses)[:,1].mean())\n",
    "            \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(val_loader):\n",
    "                # Ensure a good batch\n",
    "                batch = make_good_batch(batch)\n",
    "                \n",
    "                # Extract samples\n",
    "                image, target = batch['image'].to(device), batch['code'].to(device)\n",
    "                output = model(image.float())\n",
    "\n",
    "                # Get validation loss and predictions\n",
    "                val_loss += criterion(output, target).item()\n",
    "                predicted = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += predicted.eq(target.view_as(predicted)).sum().item() # Number of correct\n",
    "    \n",
    "                # Store the labels\n",
    "                all_labels.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "                del image, target, batch, output\n",
    "                gc.collect()\n",
    "            \n",
    "            val_loss /= len(val_loader.dataset)/val_loader.batch_size\n",
    "            val_losses_c += val_loss # add to the cumulative\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "        # Print the average training/validation loss for the current epoch\n",
    "        avg_train_loss = epoch_losses[-1]\n",
    "        avg_val_loss = val_losses_c / int(epoch+1)\n",
    "        \n",
    "        # Adjust the learning rate based on the validation loss\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}], Average Train Loss: {avg_train_loss:.4f}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Check for a learning rate increase\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr != current_lr:\n",
    "            print(f\"-- LR-Scheduler: ReduceLROnPlateau; new LR={new_lr} at epoch {epoch}\")\n",
    "        \n",
    "        # Calculate additional metrics using sklearn\n",
    "        epoch_accuracy.append(correct / len(val_loader.dataset)) \n",
    "        epoch_precision.append(precision_score(all_labels, all_predictions, average='weighted'))\n",
    "        epoch_recall.append(recall_score(all_labels, all_predictions, average='weighted'))\n",
    "        epoch_f1.append(f1_score(all_labels, all_predictions, average='weighted'))\n",
    "\n",
    "        epoch_t1 = (time.time() - epoch_t0) / 60 # minutes\n",
    "        epoch_time.append(round(epoch_t1, 2))\n",
    "\n",
    "    # Get the model averages\n",
    "    avg_accuracy = sum(epoch_accuracy) / len(epoch_accuracy)\n",
    "    avg_f1 = sum(epoch_f1) / len(epoch_f1)\n",
    "    avg_precision = sum(epoch_precision) / len(epoch_precision)\n",
    "    avg_recall = sum(epoch_recall) / len(epoch_recall)\n",
    "    avg_time = sum(epoch_time) / len(epoch_time)\n",
    "    \n",
    "    print(f\"Accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1 Score: {avg_f1:.4f}\\nTime/epoch: {avg_time}\")\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return a combination of metrics to be used in tuning\n",
    "    return {\n",
    "        'accuracy': epoch_accuracy,\n",
    "        'precision': epoch_precision,\n",
    "        'recall': epoch_recall,\n",
    "        'f1': epoch_f1,\n",
    "        'train_losses': epoch_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "\n",
    "print(\"Tuning function ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799a6c-ad3e-46df-82e1-8df3faedc146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6055219-5b01-4df8-a9fe-36c1c3cd0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Create a blank data frame to store the results\n",
    "results_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'batch_size', 'window_size', 'learning_rate', \n",
    "        'accuracy', 'precision', 'recall', 'f1', \n",
    "        'train_losses', 'val_losses', 'time'\n",
    "    ])\n",
    "\n",
    "# Perform the grid search\n",
    "for i, params in enumerate(param_list):\n",
    "    t00 = time.time()\n",
    "\n",
    "    params = param_list[i]\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "\n",
    "    # Run the trial, store the metrics\n",
    "    metrics = resnet_tuning(params)\n",
    "\n",
    "    t1 = (time.time() - t00) / 60 # minutes\n",
    "    print(f\"Elapsed time for parameter combination {i}: {t1:.2f} minutes.\")\n",
    "\n",
    "    # Store epoch-wise metrics for each trial\n",
    "    for epoch in range(len(metrics['accuracy'])):\n",
    "        results_df = results_df.append({\n",
    "            'trial': i,\n",
    "            'batch_size': params['batch_size'],\n",
    "            'window_size': params['window_size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epoch': epoch + 1,\n",
    "            'accuracy': metrics['accuracy'][epoch],\n",
    "            'precision': metrics['precision'][epoch],\n",
    "            'recall': metrics['recall'][epoch],\n",
    "            'f1': metrics['f1'][epoch],\n",
    "            'train_loss': metrics['train_losses'][epoch],\n",
    "            'val_loss': metrics['val_losses'][epoch],\n",
    "            'time': round(t1, 2),\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    print(f\"Final Accuracy: {metrics['accuracy'][-1]}\\nFinal F1-score: {metrics['f1'][-1]}\")\n",
    "    \n",
    "    # Clear unused variables\n",
    "    del metrics\n",
    "    gc.collect()  # Run garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory\n",
    "\n",
    "t2 = ((time.time() - t0) / 60) / 60\n",
    "print(f\"Total elapsed time for hyperparameter tuning: {t2:.2f} hours.\")\n",
    "\n",
    "# save the results for further analysis\n",
    "results_df.to_csv('opp-data/results/resnet18_grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285dade-9a46-4948-8e57-d7a47af86717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce607e79-e16e-4b55-8ddd-d875f79984e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the results for further analysis\n",
    "results_df.to_csv('opp-data/results/resnet18_grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9c1a4-3107-4970-816e-c9a122ddeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372c84-5493-4675-816c-c03ca020749a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061561-38d0-428b-80f2-8a07c8fc7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a87f-dc82-4d1f-b29d-ecb16a97bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
