{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a157a0-0b7e-4a0d-900c-79bc0fc3d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6dba8-e355-492b-b096-59ff723e68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected CRS: EPSG:32618\n",
      "Successfully imported all packages!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resnet-18 for classifying roof materials from PlanetScope SuperDove imagery\n",
    "Case study in Washington, D.C. \n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from torchsat.models.classification import resnet18\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "plt.ion() # interactive\n",
    "\n",
    "# Projection information\n",
    "wgs = from_epsg(4326)\n",
    "proj = from_epsg(32618)\n",
    "print(f'Projected CRS: {proj}')\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "\n",
    "print(\"Successfully imported all packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ce3bf8-31c5-456f-a965-3c7bc461ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class and functions ready to use!\n"
     ]
    }
   ],
   "source": [
    "class RoofImageDataset_Planet(Dataset):\n",
    "    \"\"\"Class to handle PlanetScope SuperDove imagery for Resnet-18\"\"\"\n",
    "\n",
    "    def __init__(self, gdf, img_path, n_bands=8, imgdim=64, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gdf: Geodataframe containing 'geometry' column and 'class_code' column\n",
    "            img_path: the path to the PlanetScope SuperDove composite image (single mosaic file)\n",
    "                - see 'psscene-prep.py' for spectral indices calculation\n",
    "            imgdim (int): Image dimension for CNN implementation\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "\n",
    "        Returns image chunks with class labels\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise ValueError(f'Image does not exists: {img_path}')\n",
    "\n",
    "        self.geometries = [p.centroid for p in gdf.geometry.values]\n",
    "        self.img = img_path\n",
    "        self.image_dim = imgdim # resnet window dimension, defaults to 64\n",
    "        self.n_bands = n_bands\n",
    "        self.Y = gdf.code.values\n",
    "        # Define transforms\n",
    "        if transform is not None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((self.image_dim, self.image_dim)),  # Resize to NxN for ResNet-18\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.geometries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Sample the PlanetScope image\n",
    "        left, bottom, right, top = self.geometries[idx].bounds\n",
    "        geom = self.geometries[idx]\n",
    "\n",
    "        try:\n",
    "            sample = self.sample_image(geom)  # run the sampling function\n",
    "            \n",
    "            # if self.transform:\n",
    "            #     sample = self.transform(sample)  # transform to common size\n",
    "\n",
    "            cc = self.Y[idx]  # get the class codes\n",
    "            \n",
    "            # Ensure the sample has the correct dimensions\n",
    "            if sample.shape[1:] != (self.image_dim, self.image_dim):\n",
    "                raise ValueError(f'Invalid sample shape: {sample.shape}')\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping invalid sample at index {idx}: {e}\")\n",
    "            return None  # Return None for invalid samples\n",
    "        \n",
    "        # Convert the sample array to a Torch object\n",
    "        sample = torch.from_numpy(sample)\n",
    "\n",
    "        return {'image': sample.type(torch.FloatTensor),\n",
    "                'code': torch.tensor(cc).type(torch.LongTensor)}\n",
    "        \n",
    "        \n",
    "    def sample_image(self, geom):\n",
    "        # Sample the image at each geometry\n",
    "        samples = [] # store the samples in a list\n",
    "\n",
    "        N = self.image_dim\n",
    "        half_N = self.image_dim / 2\n",
    "            \n",
    "        # Use the windows.from_bounds() method to return the window\n",
    "        # Returns image chunks from training data locations\n",
    "        with rio.open(self.img) as src:\n",
    "            py, px = src.index(geom.x, geom.y)\n",
    "            window = rio.windows.Window(px - N // 2, py - N // 2, N, N)\n",
    "            \n",
    "            # Read the data in the window\n",
    "            # clip is a nbands * N * N numpy array\n",
    "            clip = src.read(window=window)\n",
    "\n",
    "            # Handle the case where the sample is smaller than the expected size\n",
    "            if clip.shape != (self.n_bands, N, N):\n",
    "                raise ValueError(f'Invalid sample shape: {clip.shape}\\nAttempting to create padding ...')\n",
    "                padding = [(0, 0), (0, max(0, N - clip.shape[1])), (0, max(0, N - clip.shape[2]))]\n",
    "                clip = np.pad(clip, padding, mode='constant', constant_values=0)\n",
    "            else:\n",
    "                samples.append(clip)\n",
    "\n",
    "            del clip, py, px, window\n",
    "\n",
    "        # Convert the image chunk to a numpy array\n",
    "        samples_arr = np.array(samples)\n",
    "\n",
    "        del samples # Clear up memory\n",
    "\n",
    "        # Make sure there is valid data\n",
    "        if samples_arr.sum() > 0:\n",
    "            ans = np.ma.masked_equal(samples_arr, 0).mean(axis=0)\n",
    "        else:\n",
    "            ans = samples_arr.mean(axis=0)\n",
    "\n",
    "        del samples_arr\n",
    "\n",
    "        return ans\n",
    "\n",
    "\n",
    "def make_good_batch(batch):\n",
    "    \"\"\"\n",
    "    Removes bad samples if image dimensions do not match.\n",
    "    Args:\n",
    "        - batch: list of dictionaries, each containing 'image' tensor and 'code' tensor\n",
    "    returns: list of dictionaries same as input with samples having non-matching image dims removed\n",
    "    \"\"\"\n",
    "    valid_samples = []\n",
    "    for sample in batch:\n",
    "        if sample is not None:\n",
    "            image, code = sample['image'], sample['code']\n",
    "            if code != 255 and not torch.isnan(code) and not torch.isinf(code):\n",
    "                if not torch.isnan(image).any() and not torch.isinf(image).any():\n",
    "                    if image.shape == (8, 64, 64):  # Ensure dimensions match\n",
    "                        valid_samples.append(sample)\n",
    "            del image, code\n",
    "\n",
    "    if not valid_samples:\n",
    "        return None\n",
    "\n",
    "    return default_collate(valid_samples)\n",
    "    del valid_samples, sample\n",
    "\n",
    "\n",
    "print(\"Class and functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0a05-5aed-4233-a6f5-c06462bae509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73acf7f-7546-4aa7-880c-489596ef92dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f6824-7a2c-4d67-913e-3714fc33e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64b5e6b-0538-4cc5-8bad-d44631916569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('/home/jovyan')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308774c2-5428-4247-adb3-836626ae7255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_code</th>\n",
       "      <th>areaUTMsqft</th>\n",
       "      <th>uid</th>\n",
       "      <th>description</th>\n",
       "      <th>code</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS</td>\n",
       "      <td>357.783709</td>\n",
       "      <td>1CS</td>\n",
       "      <td>Composition Shingle</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((324215.868 4313568.665, 324215.792 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS</td>\n",
       "      <td>918.640862</td>\n",
       "      <td>2CS</td>\n",
       "      <td>Composition Shingle</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((324602.816 4311717.247, 324604.322 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS</td>\n",
       "      <td>1383.414170</td>\n",
       "      <td>3CS</td>\n",
       "      <td>Composition Shingle</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((327253.581 4300371.859, 327258.154 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS</td>\n",
       "      <td>836.410297</td>\n",
       "      <td>4CS</td>\n",
       "      <td>Composition Shingle</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((333608.13 4306267.691, 333607.957 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS</td>\n",
       "      <td>330.514264</td>\n",
       "      <td>5CS</td>\n",
       "      <td>Composition Shingle</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((326482.699 4300939.466, 326487.386 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_code  areaUTMsqft  uid          description  code  \\\n",
       "0         CS   357.783709  1CS  Composition Shingle     0   \n",
       "1         CS   918.640862  2CS  Composition Shingle     0   \n",
       "2         CS  1383.414170  3CS  Composition Shingle     0   \n",
       "3         CS   836.410297  4CS  Composition Shingle     0   \n",
       "4         CS   330.514264  5CS  Composition Shingle     0   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((324215.868 4313568.665, 324215.792 4...  \n",
       "1  POLYGON ((324602.816 4311717.247, 324604.322 4...  \n",
       "2  POLYGON ((327253.581 4300371.859, 327258.154 4...  \n",
       "3  POLYGON ((333608.13 4306267.691, 333607.957 43...  \n",
       "4  POLYGON ((326482.699 4300939.466, 326487.386 4...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data (footprints)\n",
    "# ref_path = os.path.join('data/dc_data_reference_footprints.gpkg')\n",
    "ref_path = os.path.join(maindir,'data/spatial/mod/dc_data/training/dc_data_reference_footprints.gpkg')\n",
    "ref = gpd.read_file(ref_path)\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f3f633-3a27-472d-ba32-4248eb2032c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean footprint area (sqm): 99.499113\n",
      "90th percentile footprint area (sqm): 158.13312473978425\n",
      "90th percentile side length (m): 12\n",
      "Optimal window size: 36\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 'optimal' window size from the footprint areas\n",
    "mean_area_sqft = int(ref.areaUTMsqft.values.mean())\n",
    "pct90_area_sqft = np.percentile(ref.areaUTMsqft, 90)\n",
    "print(f'Mean footprint area (sqm): {mean_area_sqft * 0.092903}')\n",
    "print(f'90th percentile footprint area (sqm): {pct90_area_sqft * 0.092903}')\n",
    "# Convert sqft to sqm\n",
    "pct90_area_sqm = pct90_area_sqft * 0.092903\n",
    "# Calculate the side length ('optimal' window size) * 3 \n",
    "print(f'90th percentile side length (m): {int(np.sqrt(pct90_area_sqm))}')\n",
    "window_size = (int(np.sqrt(pct90_area_sqm) * 3) - 1)\n",
    "print(f'Optimal window size: {window_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb845358-d7eb-4350-89fd-5ae7dbbc2c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class_code\n",
       "ME    29651\n",
       "CS    27687\n",
       "SL    11080\n",
       "UR     1018\n",
       "WS      866\n",
       "TL      617\n",
       "SH      589\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the class imbalance in the reference data\n",
    "print(\"Class counts:\\n\")\n",
    "ref.class_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a8917-595a-4e7b-9934-bf5ca4542e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18afdd2-df96-4bc6-8b64-c445134f787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_code\n",
      "ME     29651\n",
      "CS     27687\n",
      "SL     11080\n",
      "WSH     1455\n",
      "UR      1018\n",
      "TL       617\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the shingle classes (wood shingle and shingle)\n",
    "merge = {'WS': 'WSH', 'SH': 'WSH'}\n",
    "ref['class_code'].replace(merge, inplace=True)\n",
    "ref['code'], _ = pd.factorize(ref['class_code']) # create a factorized version\n",
    "print(ref['class_code'].value_counts())  # check the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d010599-d2fb-462f-9741-5b2bb9baa79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b030b0e8-3624-4946-8227-09db94678667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CS': 0, 'ME': 1, 'SL': 2, 'UR': 3, 'TL': 4, 'WSH': 5}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping class_code to code\n",
    "class_mapping = dict(zip(ref['class_code'], ref['code']))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092af89-47c4-4691-911f-7f468518d146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f55055-37f9-4faa-a915-731b49e56330",
   "metadata": {},
   "outputs": [],
   "source": [
    " Load the training GDF\n",
    "training_gdf = gpd.read_file('data/rooftop_materials_training_gdf.gpkg')\n",
    "\n",
    "# Perform balanced sampling (random undersampling)\n",
    "training_gdf_ = balance_sampling(training_gdf, ratio=10, strategy='undersample')\n",
    "\n",
    "training_gdf_.code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7758d-b7b0-4642-88e1-f70b94f25552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a0e89-b7d6-4fb2-8a39-ee3bfba17681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59df331a-d7e5-401c-8077-f5e2053039c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      " code\n",
      "1    17790\n",
      "0    16612\n",
      "2     6648\n",
      "5      873\n",
      "3      611\n",
      "4      370\n",
      "Name: count, dtype: int64\n",
      "Validation class distribution:\n",
      " code\n",
      "1    11861\n",
      "0    11075\n",
      "2     4432\n",
      "5      582\n",
      "3      407\n",
      "4      247\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test for each class\n",
    "train_df, test_df, val_df = [], [], []\n",
    "\n",
    "# Define split ratio\n",
    "vs = 0.4  # Validation size ratio (20%)\n",
    "\n",
    "# Perform stratified split to separate training data from validation data\n",
    "train_df, val_df = train_test_split(\n",
    "    ref, # your filtered samples\n",
    "    test_size=vs, \n",
    "    random_state=27, \n",
    "    stratify=ref['code']\n",
    ")\n",
    "\n",
    "# Print the class distribution in training and validation sets to verify stratification\n",
    "print(\"Train class distribution:\\n\", train_df['code'].value_counts())\n",
    "print(\"Validation class distribution:\\n\", val_df['code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ea955-ccd5-47ab-a6ef-9804207cbb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c8c5d-baea-4af6-8268-8eb2761d9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd122a-1ab3-425c-9390-4aca8b23f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca50c71-761b-48d5-95ca-e23731769ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 7555, 6046)\n",
      "shape: (7555, 6046)\n",
      "resolution: (3.0, -3.0)\n",
      "bounds: (316269.0, 4295631.0, 334407.0, 4318296.0)\n",
      "sum: 4.670212268829346\n",
      "CRS: EPSG:32618\n",
      "NoData: NoneArray: <xarray.DataArray (band: 8, y: 7555, x: 6046)> Size: 1GB\n",
      "[365420240 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * band         (band) int64 64B 1 2 3 4 5 6 7 8\n",
      "  * x            (x) float64 48kB 3.163e+05 3.163e+05 ... 3.344e+05 3.344e+05\n",
      "  * y            (y) float64 60kB 4.318e+06 4.318e+06 ... 4.296e+06 4.296e+06\n",
      "    spatial_ref  int64 8B 0\n",
      "Attributes:\n",
      "    TIFFTAG_IMAGEDESCRIPTION:  {\"atmospheric_correction\": {\"aerosol_model\": \"...\n",
      "    TIFFTAG_DATETIME:          2022:06:05 14:56:31\n",
      "    STATISTICS_APPROXIMATE:    YES\n",
      "    STATISTICS_MAXIMUM:        15203\n",
      "    STATISTICS_MEAN:           662.44066810447\n",
      "    STATISTICS_MINIMUM:        80\n",
      "    STATISTICS_STDDEV:         475.12461911944\n",
      "    STATISTICS_VALID_PERCENT:  42.96\n",
      "    AREA_OR_POINT:             Area\n",
      "    scale_factor:              1.0\n",
      "    add_offset:                0.0\n",
      "    long_name:                 ('nir', 'ndre', 'vgnir', 'vrnir', 'ndbibg', 'n...\n"
     ]
    }
   ],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join('data/dc_data_psscene15b_norm_r.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, mask=True, cache=False).squeeze()\n",
    "print(stack_da.shape)\n",
    "print(\n",
    "    f\"shape: {img.rio.shape}\\n\"\n",
    "    f\"resolution: {img.rio.resolution()}\\n\"\n",
    "    f\"bounds: {img.rio.bounds()}\\n\"\n",
    "    f\"sum: {img.sum().item()}\\n\"\n",
    "    f\"CRS: {img.rio.crs}\\n\"\n",
    "    f\"NoData: {img.rio.nodata}\"\n",
    "    f\"Array: {img}\"\n",
    ")\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225acc4-3619-4a3e-8b58-ec80b7c69f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464a06e-7e88-49d1-a9d7-8f55e36ee544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b564fc-055b-4a85-97b6-0fc49b5167f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de33d04c-8d55-4079-9ad3-c1cc4e10df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17031, 14621, 4941, 285, 151, 119]\n",
      "Class weights: tensor([  2.1812,   2.5407,   7.5183, 130.3439, 246.0132, 312.1681],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Number of samples in each class\n",
    "val_counts = list(train_df['code'].value_counts())\n",
    "print(val_counts)\n",
    "\n",
    "total_samples = sum(val_counts)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = [total_samples / count for count in val_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the calculated class weights for verification\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ffb6c-d4a0-487b-9ae1-5243892b626f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42775290-3fd6-4b70-9784-ede4e6bb2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa012653-1a9f-4e0c-acb2-b8f1b6a6025b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d065-53ff-4e61-9b0d-0d0760e3c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fun(trial):\n",
    "    \"\"\" Objective function for hyperparameter tuning \"\"\"\n",
    "    \"\"\"\n",
    "    Function for fine-tuning Resnet-18 model using 'optuna' Python package\n",
    "    Args:\n",
    "        - trial: Optuna trial\n",
    "        - tds: training dataset\n",
    "        - vds: validation dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Suggest hyperparameters to test\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    momentum = trial.suggest_uniform('momentum', 0.8, 0.99)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
    "\n",
    "    # Load the train, test, and validation\n",
    "    train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=True, collate_fn=make_good_batch)\n",
    "    val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False, collate_fn=make_good_batch)\n",
    "\n",
    "    # Model definition\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(in_features, N_classes)  # NN = number of classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=Class_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Adjust number of epochs as needed\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799a6c-ad3e-46df-82e1-8df3faedc146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cad24-4dad-4fa9-ac93-e88209ba7e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95d9c1a4-3107-4970-816e-c9a122ddeaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372c84-5493-4675-816c-c03ca020749a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911efed-945d-49e3-a6ea-a2800e4bc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_fun, n_trials=50)\n",
    "\n",
    "t1 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t1:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d0fc8-11b3-4ae8-9771-2ce256d4dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best hyperparameters and accuracy\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061561-38d0-428b-80f2-8a07c8fc7f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a87f-dc82-4d1f-b29d-ecb16a97bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7242b8aa-59fd-4eef-98e4-762e40b4740e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
