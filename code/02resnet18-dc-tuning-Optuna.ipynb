{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6dba8-e355-492b-b096-59ff723e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter optimization for Resnet-18 classification of roof materials using 'Optuna'\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, time, glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "from torchsat.models.classification import resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "from shapely.geometry import box\n",
    "from os.path import join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "\n",
    "# Projection information\n",
    "wgs = from_epsg(4326)\n",
    "proj = from_epsg(32618)\n",
    "print(f'Projected CRS: {proj}')\n",
    "\n",
    "# maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "\n",
    "print(\"Successfully imported all packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3bf8-31c5-456f-a965-3c7bc461ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoofImageDatasetPlanet(Dataset):\n",
    "    \"\"\"Class to handle PlanetScope SuperDove imagery for Resnet-18\"\"\"\n",
    "\n",
    "    def __init__(self, gdf, img_path, n_bands, img_dim, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gdf: Geodataframe containing 'geometry' column and 'class_code' column\n",
    "            img_path: the path to the PlanetScope SuperDove composite image (single mosaic file)\n",
    "                - see 'psscene-prep.py' for spectral indices calculation\n",
    "            imgdim (int): Image dimension for CNN implementation\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "\n",
    "        Returns image chunks with class labels\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise ValueError(f'Image does not exists: {img_path}')\n",
    "\n",
    "        self.geometries = [p.centroid for p in gdf.geometry.values]  # gather centroid geoms\n",
    "        self.img_path = img_path  # path to image data\n",
    "        self.img_dim = img_dim  # resnet window dimension, defaults to 64\n",
    "        self.n_bands = n_bands  # number of bands in the input image\n",
    "        self.Y = gdf.code.values  # class codes (numeric)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.geometries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the geometry of the idx (centroid)\n",
    "        geom = self.geometries[idx]\n",
    "\n",
    "        try:\n",
    "            sample = self.sample_image(geom)  # run the sampling function\n",
    "\n",
    "            cc = self.Y[idx]  # get the class codes\n",
    "            if type(cc) != int:\n",
    "                cc = cc.astype('uint8')  # make sure the cc is an integer\n",
    "\n",
    "            # Ensure the sample has the correct dimensions\n",
    "            assert sample.shape == (self.n_bands, self.img_dim, self.img_dim), f'Invalid sample shape: {sample.shape}'\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "            print(f\"Skipping invalid sample at index: {idx}\")\n",
    "            sample = torch.from_numpy(np.zeros((self.n_bands, int(self.img_dim), int(self.img_dim))))\n",
    "            cc = 255  # highest int8 number to be flagged\n",
    "\n",
    "        # Convert the sample array to a Torch object\n",
    "        sample = torch.from_numpy(sample)\n",
    "\n",
    "        # Return the sample and the label as torch objects\n",
    "        return {'image': sample.type(torch.FloatTensor),\n",
    "                'code': torch.tensor(cc).type(torch.LongTensor)}\n",
    "\n",
    "    def sample_image(self, geom):\n",
    "        \"\"\" Sample the image at each geometry for the specified image chunk size (window) \"\"\"\n",
    "\n",
    "        N = self.img_dim  # window size to be used for cropping\n",
    "\n",
    "        # Use the windows.from_bounds() method to return the window\n",
    "        # Returns image chunks from training data locations\n",
    "        with rio.open(self.img_path) as src:\n",
    "            py, px = src.index(geom.x, geom.y)\n",
    "            window = rio.windows.Window(px - N // 2, py - N // 2, N, N)\n",
    "            # print(window)\n",
    "\n",
    "            # Read the data in the window\n",
    "            # clip is a nbands * N * N numpy array\n",
    "            clip = src.read(window=window, indexes=list(range(1, self.n_bands + 1)))\n",
    "\n",
    "            del py, px, window  # clean up\n",
    "\n",
    "        # Convert the image chunk to a numpy array\n",
    "        clip_arr = np.array(clip)\n",
    "\n",
    "        # Check if the image chunk has valid data\n",
    "        if clip_arr.sum() > 0:\n",
    "            # Mask invalid values in each band independently\n",
    "            ans = np.ma.masked_equal(clip_arr, 0).filled(0)\n",
    "        else:\n",
    "            ans = clip_arr\n",
    "\n",
    "        del clip, clip_arr  # clean up\n",
    "        return ans\n",
    "\n",
    "\n",
    "def make_good_batch(batch):\n",
    "    \"\"\"\n",
    "    Removes bad samples if image dimensions do not match.\n",
    "    Args:\n",
    "        - batch: list of dictionaries, each containing 'image' tensor and 'code' tensor\n",
    "    returns: list of dictionaries same as input with samples having non-matching image dims removed\n",
    "    \"\"\"\n",
    "\n",
    "    _idx = torch.where(batch['code'] != 255)[0]  # good batches\n",
    "\n",
    "    new_batch = {}\n",
    "    new_batch['image'] = batch['image'][_idx]\n",
    "    new_batch['code'] = batch['code'][_idx]\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "print(\"Functions ready !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0a05-5aed-4233-a6f5-c06462bae509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73acf7f-7546-4aa7-880c-489596ef92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan')\n",
    "print(os.getcwd())\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f6824-7a2c-4d67-913e-3714fc33e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c9d1d-4ffc-4f77-9a02-6b06decee337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the footprint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84e598-669e-4c2e-8e6c-4feb1635a08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308774c2-5428-4247-adb3-836626ae7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (footprints)\n",
    "ref_path = 'opp-data/dc_data_reference_footprints.gpkg'\n",
    "ref = gpd.read_file(ref_path)\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845358-d7eb-4350-89fd-5ae7dbbc2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the class imbalance in the reference data\n",
    "print(f\"Class counts:\\n\\n{ref.description.value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750bbca-52e9-4e68-8850-3ecf9995d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref['code'], _ = pd.factorize(ref['class_code']) # create a factorized version\n",
    "print(ref['class_code'].value_counts())  # check the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90464cdd-b12f-4d9a-a448-456858b17220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping class_code to code\n",
    "code_mapping = dict(zip(ref['class_code'], ref['code']))\n",
    "desc_mapping = dict(zip(ref['class_code'], ref['description']))\n",
    "print(f'Code map: \\n{code_mapping}\\nDescription map: \\n{desc_mapping}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482031cc-1d68-4426-8709-6bbf1ad4eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform balanced sampling (random undersampling)\n",
    "ref_bal = balance_sampling(ref, ratio=20, strategy='undersample')\n",
    "ref_bal.code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030b0e8-3624-4946-8227-09db94678667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/test data\n",
    "train_df, val_df, test_df = split_training_data(ref_bal, ts=0.4, vs=0.2)\n",
    "\n",
    "# Print the class distribution in training and validation sets to verify stratification\n",
    "print(\"Train class distribution:\\n\", train_df['code'].value_counts())\n",
    "print(\"Validation class distribution:\\n\", val_df['code'].value_counts())\n",
    "print(\"Test class distribution:\\n\", test_df['code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7758d-b7b0-4642-88e1-f70b94f25552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a0e89-b7d6-4fb2-8a39-ee3bfba17681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model parameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c22fa-5f03-49be-b08b-b47275bdffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca50c71-761b-48d5-95ca-e23731769ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join('opp-data/dc_0623_psscene8b_final_norm.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, mask=True, cache=False).squeeze()\n",
    "n_bands = stack_da.values.shape[:1][0]\n",
    "print(\n",
    "    f\"shape: {stack_da.rio.shape}\\n\"\n",
    "    f\"bands: {n_bands}\\n\"\n",
    "    f\"resolution: {stack_da.rio.resolution()}\\n\"\n",
    "    f\"bounds: {stack_da.rio.bounds()}\\n\"\n",
    "    f\"sum: {stack_da.sum().item()}\\n\"\n",
    "    f\"CRS: {stack_da.rio.crs}\\n\"\n",
    "    f\"NoData: {stack_da.rio.nodata}\\n\"\n",
    "    f\"Array: {stack_da}\"\n",
    ")\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225acc4-3619-4a3e-8b58-ec80b7c69f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464a06e-7e88-49d1-a9d7-8f55e36ee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Resnet-18 model\n",
    "\n",
    "# Define whether to leverage cpu or gpu (for my local machine it is only cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # get device for gpu or cpu\n",
    "print(f'Using {device} for model dev ...')\n",
    "\n",
    "# Grab the number of classes and bands\n",
    "n_classes = ref_bal.class_code.unique().shape[0]\n",
    "print(f'There are {n_classes} roof type classes.')\n",
    "print(f'Using {n_bands} bands for classification.')\n",
    "\n",
    "# Define the Resnet-18 model (in_channels = number of bands in the image)\n",
    "model = resnet18(n_classes, in_channels=n_bands, pretrained=False)\n",
    "\n",
    "# Make model parallel and on GPU\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    #ps_model = nn.DataParallel(ps_model)\n",
    "    model = nn.DataParallel(model)\n",
    "    print('Made cpu parallel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33d04c-8d55-4079-9ad3-c1cc4e10df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in each class\n",
    "val_counts = list(train_df['code'].value_counts())\n",
    "total_samples = sum(val_counts) # total number of samples\n",
    "print(f'Total samples: {total_samples};\\nValue counts: {val_counts}')\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = [total_samples / count for count in val_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "class_weights_norm = class_weights / class_weights.sum()\n",
    "print(f\"Normalized class weights: {class_weights_norm}\")\n",
    "\n",
    "# Updated loss function with weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_norm).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ffb6c-d4a0-487b-9ae1-5243892b626f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42775290-3fd6-4b70-9784-ede4e6bb2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa012653-1a9f-4e0c-acb2-b8f1b6a6025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train DataFrame indices: {val_df.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d065-53ff-4e61-9b0d-0d0760e3c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir = stack_da_fp\n",
    "\n",
    "window_size = 64\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\" \n",
    "    Function for fine-tuning Resnet-18 model using 'optuna' Python package\n",
    "    Args:\n",
    "        - trial: Optuna trial\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Suggest hyperparameters to test\n",
    "        batch_size = trial.suggest_int('batch_size', 128, 256)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed due to: {e}\")\n",
    "        return None  # Returning None to indicate a failed trial\n",
    "        \n",
    "    # Load the train, test, and validation\n",
    "\n",
    "    # Train\n",
    "    \n",
    "    # Create the training samples\n",
    "    train_ds = RoofImageDataset_Planet(train_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Create the validation samples\n",
    "    val_ds = RoofImageDataset_Planet(val_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Adjust number of epochs as needed\n",
    "        running_loss = 0.0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # Ensure a good batch\n",
    "            batch = make_good_batch(batch)\n",
    "            \n",
    "            inputs, labels = batch['image'].to(device), batch['code'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            # Ensure a good batch\n",
    "            batch = make_good_batch(batch)\n",
    "            \n",
    "            inputs, labels = batch['image'].to(device), batch['code'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"Ready !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799a6c-ad3e-46df-82e1-8df3faedc146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cad24-4dad-4fa9-ac93-e88209ba7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "t0 = time.time()\n",
    "\n",
    "study = optuna.create_study(study_name=\"Resnet-18 Hyperparameter Tuning\", direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Display the best hyperparameters and accuracy\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "t1 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t1:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9c1a4-3107-4970-816e-c9a122ddeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372c84-5493-4675-816c-c03ca020749a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061561-38d0-428b-80f2-8a07c8fc7f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a87f-dc82-4d1f-b29d-ecb16a97bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
