{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd450a-7dd2-438a-ba05-a26148aa6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost multiclassifier for roof materials in D.C.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494fcfe-32ef-4f57-9161-d830e4e09c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions!\n",
    "\n",
    "def compute_band_stats(geom, band, stat, nodataval):\n",
    "    \"\"\"\n",
    "    Computes band statistics for given geometries\n",
    "\n",
    "    Args:\n",
    "        geom: geometries from which to sample image data\n",
    "        band: the band to calculate statistics\n",
    "        stat: which statostoic to be used (inherited)\n",
    "        nodataval: the No Data value to be used\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Check the geometry type to determine which method to use\n",
    "    if (geom.geometry.geom_type).isin(['Polygon', 'MultiPolygon']):\n",
    "    \n",
    "        stats = zonal_stats(\n",
    "            self.geometries,\n",
    "            self.img,\n",
    "            stats=[stat],\n",
    "            band_num=band,\n",
    "            all_touched=True,\n",
    "            nodata=nodataval,\n",
    "            geojson_out=False\n",
    "        )\n",
    "        \n",
    "        return {band: [feature['properties'][stat] for feature in stats]}\n",
    "\n",
    "    else:\n",
    "\n",
    "        coord_list = [(x, y) for x, y in zip(geom[\"geometry\"].x, geom[\"geometry\"].y)]\n",
    "        \n",
    "        points[f\"{band}\"] = [x for x in img.sample(coord_list, indexes=i+1)]\n",
    "        \n",
    "        points_df = points.reset_index()\n",
    "        \n",
    "        points_df[desc] = points_df[band_names].astype(np.float32)\n",
    "        \n",
    "        return points_df\n",
    "        \n",
    "        \n",
    "def sample_image_da(img_path, geom, stat='mean'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Sample the image at each geometry    \n",
    "    # Create a copy of the polygons to store the results\n",
    "    stats_df = geom.copy()\n",
    "\n",
    "    # Number of bands to be processed\n",
    "    n_bands = img.count\n",
    "    band_names = img.long_name\n",
    "    nodataval = img.nodata\n",
    "\n",
    "    # Calculate the number of cores to use, reserving 2 cores\n",
    "    num_cores = os.cpu_count()\n",
    "    if num_cores is not None:  # os.cpu_count() can return None\n",
    "        max_workers = max(1, num_cores - 1)  # Reserve 2 cores, but ensure at least 1 worker\n",
    "    else:\n",
    "        max_workers = 1  # Default to 1 worker if os.cpu_count() is None\n",
    "\n",
    "    # Set up parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for band, band_name in zip(n_bands, band_names):\n",
    "            print(f\"Processing for {band}.\")\n",
    "            futures.append(executor.submit(compute_band_stats, band, img_path, geom, stat, nodataval))\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            band = list(result.keys())[0]\n",
    "            stats_df[f'band_{band}'] = result[band]\n",
    "\n",
    "    # Optionally, rename columns based on band names\n",
    "    band_name_mapping = {f'band_{i + 1}': name for i, name in enumerate(band_names)}\n",
    "    stats_df.rename(columns=band_name_mapping, inplace=True)\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "    \n",
    "def print_raster(raster, open_file):\n",
    "    \"\"\"\n",
    "    :param raster: input raster file\n",
    "    :param open_file: should the file be opened or not\n",
    "    :return: print statement with raster information\n",
    "    \"\"\"\n",
    "    if open_file is True:\n",
    "        img = rxr.open_rasterio(raster,masked=True, cache=False).squeeze()\n",
    "    else:\n",
    "        img = raster\n",
    "    print(\n",
    "        f\"shape: {img.rio.shape}\\n\"\n",
    "        f\"resolution: {img.rio.resolution()}\\n\"\n",
    "        f\"bounds: {img.rio.bounds()}\\n\"\n",
    "        f\"sum: {img.sum().item()}\\n\"\n",
    "        f\"CRS: {img.rio.crs}\\n\"\n",
    "        f\"NoData: {img.rio.nodata}\"\n",
    "        f\"Array: {img}\"\n",
    "    )\n",
    "    del img\n",
    "\n",
    "\n",
    "# Apply a minimum distance sample to training data\n",
    "def min_dist_sample(gdf, min_distance):\n",
    "    \"\"\"\n",
    "    Filters the GeoDataFrame to ensure samples are at least min_distance apart.\n",
    "\n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing 'geometry' column.\n",
    "        min_distance: Minimum distance between samples in the same units as the geometry.\n",
    "\n",
    "    Returns:\n",
    "        Filtered GeoDataFrame.\n",
    "    \"\"\"\n",
    "    coords = np.array([[geom.centroid.x, geom.centroid.y] for geom in gdf.geometry])\n",
    "    tree = KDTree(coords)\n",
    "    indices_to_keep = set(range(len(gdf)))\n",
    "\n",
    "    for i in range(len(gdf)):\n",
    "        if i not in indices_to_keep:\n",
    "            continue\n",
    "        indices = tree.query_radius([coords[i]], r=min_distance)[0]\n",
    "        for index in indices:\n",
    "            if index != i:\n",
    "                indices_to_keep.discard(index)\n",
    "\n",
    "    del coords, tree, indices\n",
    "\n",
    "    return gdf.iloc[list(indices_to_keep)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8fe2e-8c01-4730-85ce-33d3db2977c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join(maindir,'data/spatial/mod/dc_data/planet-data/dc_data_psscene15b.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, masked=True, cache=False).squeeze()\n",
    "print_raster(stack_da, open_file=False)\n",
    "band_names = stack_da.long_name\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e3a84-ad5e-4f77-9ea1-f6f00b044983",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = list(band_names)\n",
    "band_names.append('class_code')\n",
    "band_names.append('uid')\n",
    "band_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a29ed-8971-46dc-93ce-aea7781eb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the training data (footprints)\n",
    "# gdf_path = os.path.join(maindir,'data/spatial/mod/dc_data/training/dc_data_reference_footprints.gpkg')\n",
    "# ref = gpd.read_file(gdf_path)\n",
    "# footprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896824c9-bff9-4129-907f-e65840fd6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (sampled building footprints)\n",
    "######################################################\n",
    "\n",
    "# Footprints (from \"sample-stack.py\")\n",
    "ref_tbl_path_fp = os.path.join(maindir,'data/tabular/mod/dc_data/training/dc_data_reference_sampled_footprint.csv')\n",
    "ref = pd.read_csv(ref_tbl_path_fp)\n",
    "\n",
    "# Retain samples of band matches\n",
    "ref = ref[band_names]\n",
    "\n",
    "# Create a numeric class code\n",
    "class_mapping = {label: idx for idx, label in enumerate(ref['class_code'].unique())}\n",
    "ref['Y'] = ref['class_code'].map(class_mapping)\n",
    "\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838e9a2-12ce-4347-87aa-2e6a10929a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref['class_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78ee0d-c08f-4af0-9e8e-1bbfadcf66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Set up the model data\n",
    "y = ref['Y']\n",
    "X = ref.drop(['class_code', 'uid', 'Y'], axis=1)\n",
    "\n",
    "# Define dataframes to store results for this feature set\n",
    "results = pd.DataFrame()  # to store the model performance metrics\n",
    "feat_imps = pd.DataFrame()  # to store the feature importances\n",
    "prob_preds = pd.DataFrame()  # for testing optimum cutoff\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = y.value_counts()\n",
    "total_samples = len(y)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "print(f'Class weights: {class_weights}')\n",
    "\n",
    "# Set up the stratified K-fold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop the folds\n",
    "fold_idx = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(f'Fold: {fold_idx}')\n",
    "\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Map class weights to each sample in the training set\n",
    "    sample_weights = y_train.map(class_weights).values\n",
    "    # print(sample_weights)\n",
    "\n",
    "    # Initialize the XGBoost classifier for multi-class classification\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        n_estimators=1001,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    # Store feature importance\n",
    "    fold_imps = pd.DataFrame({\n",
    "        'Fold': fold_idx,\n",
    "        'Feature': X.columns,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    })\n",
    "\n",
    "    feat_imps = pd.concat([feat_imps, fold_imps], axis=0)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Retrieve the accuracy/performance metrics\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store the metrics into the results data frame\n",
    "    fold_results = pd.DataFrame({\n",
    "        'Fold': [fold_idx],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1],\n",
    "        'MCC': [mcc],\n",
    "        'Accuracy': [accuracy]\n",
    "    })\n",
    "    results = pd.concat([results, fold_results], ignore_index=True)\n",
    "\n",
    "    # Store the probability values for cutoff testing\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)\n",
    "\n",
    "    # Store probabilities and true labels\n",
    "    fold_probs = pd.DataFrame({\n",
    "        'TrueLabel': y_test,\n",
    "        'PredictedProb': list(y_pred_proba),\n",
    "        'Fold': fold_idx\n",
    "    })\n",
    "    prob_preds = pd.concat([prob_preds, fold_probs], ignore_index=True)\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "    del fold_probs, fold_results, fold_imps\n",
    "\n",
    "    t1 = (time.time() - t0) / 60\n",
    "    print(f\"Total elapsed time for fold {fold_idx}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t2:.2f} minutes.\")\n",
    "\n",
    "# # Append the feature set-specific results to the overall results dataframes\n",
    "# all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "# all_feat_imps = pd.concat([all_feat_imps, feat_imps], ignore_index=True)\n",
    "# all_prob_preds = pd.concat([all_prob_preds, prob_preds], ignore_index=True)\n",
    "\n",
    "# del results, feat_imps, prob_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca468c06-2dc4-4525-b23f-2237436c146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118160d7-5f09-45a0-95b9-f634ef04e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine true labels and predicted labels across all folds\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "for fold in range(1, fold_idx):\n",
    "    fold_data = prob_preds[prob_preds['Fold'] == fold]\n",
    "    all_true_labels.extend(fold_data['TrueLabel'])\n",
    "    all_pred_labels.extend(np.argmax(np.vstack(fold_data['PredictedProb']), axis=1))\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(ax=ax)\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.savefig(os.path.join(maindir,'figures/FigX_xgboost_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89e8e9-2e32-4b96-811a-7357330c5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a classification report and convert it to a DataFrame\n",
    "cor_labels = [label for label, idx in sorted(class_mapping.items(), key=lambda item: item[1])]\n",
    "cr_df = pd.DataFrame(classification_report(all_true_labels, all_pred_labels, target_names=cor_labels, output_dict=True)).transpose()\n",
    "\n",
    "# Compute the average accuracy metrics across the 10 folds\n",
    "average_metrics = cr_df.loc[cor_labels].mean()\n",
    "\n",
    "# Display the DataFrame\n",
    "cr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515aca9-c573-42f1-8bd9-f6f5295fef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the results\n",
    "results.to_csv(os.path.join(maindir,'xgboost_folds_results.csv'))\n",
    "feat_imps.to_csv(os.path.join(maindir,'xgboost_folds_feat_imps.csv'))\n",
    "prob_preds.to_csv(os.path.join(maindir,'xgboost_folds_prob_peds.csv'))\n",
    "cr_df.to_csv(os.path.join(maindir,'xgboost_classification_report_avg.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
