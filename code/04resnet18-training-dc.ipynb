{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a157a0-0b7e-4a0d-900c-79bc0fc3d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6dba8-e355-492b-b096-59ff723e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet-18 for classifying roof materials from PlanetScope SuperDove imagery\n",
    "Case study in Washington, D.C. \n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "from torchsat.models.classification import resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "from shapely.geometry import box\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion() # interactive\n",
    "\n",
    "# Projection information\n",
    "wgs = from_epsg(4326)\n",
    "proj = from_epsg(32618)\n",
    "print(f'Projected CRS: {proj}')\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "\n",
    "print(\"Successfully imported all packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3bf8-31c5-456f-a965-3c7bc461ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoofImageDataset_Planet(Dataset):\n",
    "    \"\"\"Class to handle PlanetScope SuperDove imagery for Resnet-18\"\"\"\n",
    "\n",
    "    def __init__(self, gdf, img_path, n_bands, img_dim, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gdf: Geodataframe containing 'geometry' column and 'class_code' column\n",
    "            img_path: the path to the PlanetScope SuperDove composite image (single mosaic file)\n",
    "                - see 'psscene-prep.py' for spectral indices calculation\n",
    "            imgdim (int): Image dimension for CNN implementation\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "\n",
    "        Returns image chunks with class labels\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise ValueError(f'Image does not exists: {img_path}')\n",
    "\n",
    "        self.geometries = [p.centroid for p in gdf.geometry.values] # gather centroid geoms\n",
    "        self.img_path = img_path # path to image data\n",
    "        self.img_dim = img_dim # resnet window dimension, defaults to 64\n",
    "        self.n_bands = n_bands # number of bands in the input image\n",
    "        self.Y = gdf.code.values # class codes (numeric)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.geometries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the geometry of the idx (centroid)\n",
    "        geom = self.geometries[idx]\n",
    "\n",
    "        try:\n",
    "            sample = self.sample_image(geom)  # run the sampling function\n",
    "        \n",
    "            cc = self.Y[idx]  # get the class codes\n",
    "            if type(cc) != int:\n",
    "                cc = cc.astype('uint8') # make sure the cc is an integer\n",
    "            \n",
    "            # Ensure the sample has the correct dimensions\n",
    "            assert sample.shape == (self.n_bands, self.img_dim, self.img_dim), f'Invalid sample shape: {sample.shape}'\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "            print(f\"Skipping invalid sample at index: {idx}\")\n",
    "            sample = torch.from_numpy(np.zeros((self.n_bands, int(self.img_dim), int(self.img_dim))))\n",
    "            cc = 255 # highest int8 number to be flagged\n",
    "         \n",
    "        # Convert the sample array to a Torch object\n",
    "        sample = torch.from_numpy(sample)\n",
    "\n",
    "        # Return the sample and the label as torch objects\n",
    "        return {'image': sample.type(torch.FloatTensor),\n",
    "                'code': torch.tensor(cc).type(torch.LongTensor)}\n",
    "\n",
    "    \n",
    "    def sample_image(self, geom):\n",
    "        \"\"\" Sample the image at each geometry for the specified image chunk size (window) \"\"\"\n",
    " \n",
    "        N = self.img_dim # window size to be used for cropping\n",
    "            \n",
    "        # Use the windows.from_bounds() method to return the window\n",
    "        # Returns image chunks from training data locations\n",
    "        with rio.open(self.img_path) as src:\n",
    "            py, px = src.index(geom.x, geom.y)\n",
    "            window = rio.windows.Window(px - N // 2, py - N // 2, N, N)\n",
    "            # print(window)\n",
    "            \n",
    "            # Read the data in the window\n",
    "            # clip is a nbands * N * N numpy array\n",
    "            clip = src.read(window=window, indexes=list(range(1, self.n_bands + 1)))\n",
    "\n",
    "            del py, px, window # clean up\n",
    "\n",
    "        # Convert the image chunk to a numpy array\n",
    "        clip_arr = np.array(clip)\n",
    "\n",
    "        # Check if the image chunk has valid data\n",
    "        if clip_arr.sum() > 0:\n",
    "            # Mask invalid values in each band independently\n",
    "            ans = np.ma.masked_equal(clip_arr, 0).filled(0)\n",
    "        else:\n",
    "            ans = clip_arr\n",
    "        \n",
    "        del clip, clip_arr # clean up\n",
    "        return ans\n",
    "\n",
    "\n",
    "def make_good_batch(batch):\n",
    "    \"\"\"\n",
    "    Removes bad samples if image dimensions do not match.\n",
    "    Args:\n",
    "        - batch: list of dictionaries, each containing 'image' tensor and 'code' tensor\n",
    "    returns: list of dictionaries same as input with samples having non-matching image dims removed\n",
    "    \"\"\"\n",
    "\n",
    "    _idx = torch.where(batch['code'] != 255)[0] # good batches\n",
    "\n",
    "    new_batch = {}\n",
    "    new_batch['image'] = batch['image'][_idx]\n",
    "    new_batch['code'] = batch['code'][_idx]\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def balance_sampling(df, ratio=5, strategy='undersample'):\n",
    "    \"\"\"\n",
    "    Generate balanced sample from training data based on the defined ratio.\n",
    "    This can be done with majority undersampling or minority oversampling ('strategy' parameter)\n",
    "    Args:\n",
    "        - df: the dataframe with rows as training data\n",
    "        - ratio: the sampling ration (i.e., 5:1 for minority classes default)\n",
    "    Returns:\n",
    "        - random sample with class ratios as defined\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the class counts\n",
    "    class_counts = df['class_code'].value_counts()\n",
    "    min_class_count = class_counts.min()\n",
    "    \n",
    "    # Calculate the target count for each class based on the ratio\n",
    "    target_count = {\n",
    "        class_label: max(min_class_count, min(min_class_count * ratio, len(df[df['class_code'] == class_label]))) \n",
    "        for class_label in class_counts.index\n",
    "    }\n",
    "    \n",
    "    # Create an empty list to store balanced dataframes\n",
    "    balanced_dfs = []\n",
    "    for class_label in class_counts.index:\n",
    "        class_df = df[df['class_code'] == class_label]\n",
    "        if strategy == 'undersample':\n",
    "            # Under-sample the majority class, but do not undersample below its original count\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=False, n_samples=target_count[class_label], random_state=42)\n",
    "        elif strategy == 'oversample':\n",
    "            # Over-sample the minority class\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=True, n_samples=target_count[class_label], random_state=42)\n",
    "        balanced_dfs.append(balanced_class_df)\n",
    "\n",
    "    # Concatenate the results by class\n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def split_training_data(gdf, ts, vs):\n",
    "    \"\"\" \n",
    "    Splits dataframe into train, test, and validation samples with the defined ratios \n",
    "    Args:\n",
    "        - gdf: training samples (geo data frame)\n",
    "        - ts: test size #\n",
    "        - vs: validation size #\n",
    "    Returns:\n",
    "        train, test, and validation dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df, test_df, val_df = [], [], []\n",
    "\n",
    "    for cl in gdf.class_code.unique():\n",
    "        \n",
    "        # subset to class\n",
    "        _gdf = gdf.loc[gdf.class_code == cl]\n",
    "        \n",
    "        # get train and test validation arrays. \n",
    "        # test array is validation array split in half.\n",
    "        _train, _valtest = train_test_split(_gdf, random_state=27, test_size=ts)\n",
    "        train_df.append(_train)\n",
    "        \n",
    "        _val, _test = train_test_split(_valtest, random_state=27, test_size=vs)\n",
    "        test_df.append(_test)\n",
    "        val_df.append(_val)\n",
    "\n",
    "    # Concatenate the samples across classes\n",
    "    all_train_df = pd.concat(train_df)\n",
    "    all_train_df = gpd.GeoDataFrame(all_train_df, crs=gdf.crs)\n",
    "    \n",
    "    all_val_df = pd.concat(val_df)\n",
    "    all_val_df = gpd.GeoDataFrame(all_val_df, crs=gdf.crs)\n",
    "    \n",
    "    all_test_df = pd.concat(test_df)\n",
    "    all_test_df = gpd.GeoDataFrame(all_test_df, crs=gdf.crs)\n",
    "\n",
    "    return all_train_df, all_val_df, all_test_df\n",
    "\n",
    "\n",
    "print(\"Class and functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698eaec-2a8f-44a9-af31-656cf38de292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f0db2-184a-4589-b258-7283e30f23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/home/jovyan')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d53102-f55f-43ae-a20d-3bf28ce5de9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308774c2-5428-4247-adb3-836626ae7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (footprints)\n",
    "gdf_path = os.path.join('opp/data/dc_data_reference_footprints.gpkg')\n",
    "footprints = gpd.read_file(gdf_path)\n",
    "footprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3f633-3a27-472d-ba32-4248eb2032c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 'optimal' window size from the footprint areas\n",
    "mean_area_sqft = int(footprints.areaUTMsqft.values.mean())\n",
    "pct90_area_sqft = np.percentile(footprints.areaUTMsqft, 90)\n",
    "print(f'Mean footprint area (sqm): {mean_area_sqft * 0.092903}')\n",
    "print(f'90th percentile footprint area (sqm): {pct90_area_sqft * 0.092903}')\n",
    "\n",
    "# Convert sqft to sqm\n",
    "pct90_area_sqm = pct90_area_sqft * 0.092903\n",
    "\n",
    "# Calculate the side length ('optimal' window size) * 3 \n",
    "print(f'90th percentile side length (m): {int(np.sqrt(pct90_area_sqm))}')\n",
    "window_size = (int(np.sqrt(pct90_area_sqm) * 3) - 1)\n",
    "print(f'Optimal window size: {window_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845358-d7eb-4350-89fd-5ae7dbbc2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the class imbalance\n",
    "footprints.class_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18afdd2-df96-4bc6-8b64-c445134f787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SH and WS\n",
    "ref = footprints\n",
    "# Merge the shingle classes (wood shingle and shingle)\n",
    "merge = {'WS': 'WSH', 'SH': 'WSH'}\n",
    "ref['class_code'].replace(merge, inplace=True)\n",
    "print(ref['class_code'].value_counts())  # check the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfad719-f3c2-48df-aa52-66ee519b11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numeric code column\n",
    "ref['code'], _ = pd.factorize(ref['class_code'])\n",
    "# Create a dictionary mapping class_code to code\n",
    "class_mapping = dict(zip(ref['class_code'], ref['code']))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bde40-f92c-46ad-bce3-6b40075e4b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f5979-0193-46e0-ab26-1e7449d2714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify 'good' training locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c94b42-ea39-4726-abd2-6cfa775b636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c96b1-7be7-4baa-9584-6e19c28e41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centroids\n",
    "ref_pt = ref.copy()\n",
    "ref_pt = ref_pt.to_crs(epsg=32618)\n",
    "ref_pt['geometry'] = ref_pt['geometry'].centroid\n",
    "\n",
    "# Define the window size and half window (for boxes)\n",
    "window_size = 64 # for now?\n",
    "half_window = window_size / 2\n",
    "\n",
    "training_windows = [] # image windows with >50% of specific roof type\n",
    "training_roof_types = [] # roof type codes for valid windows\n",
    "\n",
    "# Loop through each footprint individually\n",
    "for geom, roof_type in zip(ref.geometry, ref['class_code']):\n",
    "\n",
    "    centroid = geom.centroid # footprint centroid\n",
    "    \n",
    "    # calculate the image window (64x64)\n",
    "    window = box(centroid.x - half_window, centroid.y - half_window,\n",
    "                 centroid.x + half_window, centroid.y + half_window)\n",
    "\n",
    "    # Intersect with centroids to get class count within window\n",
    "    intersect = ref_pt[ref_pt.intersects(window)]\n",
    "    \n",
    "    # Get the total count and count for the class\n",
    "    total_count = len(intersect)\n",
    "    class_count = len(intersect[intersect['class_code'] == roof_type])\n",
    "\n",
    "    # Check if there is at least 50% of the roof type in that window\n",
    "    if total_count > 0 and (class_count / total_count) > 0.50:\n",
    "        training_windows.append(centroid)\n",
    "        training_roof_types.append(roof_type)\n",
    "\n",
    "    del intersect, window, centroid\n",
    "\n",
    "# Create a GeoDataFrame for the training windows with roof types\n",
    "ref_windows = gpd.GeoDataFrame({\n",
    "    'geometry': training_windows, \n",
    "    'class_code': training_roof_types\n",
    "}, crs=ref.crs)\n",
    "\n",
    "# Create a numeric code for the training data frame\n",
    "ref_windows['code'], _ = pd.factorize(ref_windows['class_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf332f4-7848-47f2-8a23-784101265939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out\n",
    "os.getcwd()\n",
    "out_file = os.path.join('opp/data/rooftop_materials_training_windows.gpkg')\n",
    "ref_windows.to_file(out_file)\n",
    "\n",
    "del training_windows, training_roof_types, ref_pt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edde283-208e-4368-ad7b-92c4c2ad097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training locations with colors based on roof type\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ref_windows.plot(column='class_code', ax=ax, legend=True, cmap='Set1', edgecolor='black')\n",
    "plt.title('Training Locations by Roof Material Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a09fb4-3432-4cb6-9de6-89728c147413",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_windows.class_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195a3a1-e907-4f3e-87a4-ce28d6001e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform balanced sampling (random undersampling)\n",
    "ref_bal = balance_sampling(ref_windows, ratio=10, strategy='undersample')\n",
    "ref_bal.class_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4483537-a9f9-4008-b8e0-049dc6d924e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124813b-03f5-4a1f-8db0-269f000739e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d138d-4766-407b-8d08-0bedbcd88db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df331a-d7e5-401c-8077-f5e2053039c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/test data\n",
    "train_df, val_df, test_df = split_training_data(ref_bal, ts=0.4, vs=0.2)\n",
    "\n",
    "# Print the class distribution in training and validation sets to verify stratification\n",
    "print(\"Train class distribution:\\n\", train_df['code'].value_counts())\n",
    "print(\"Validation class distribution:\\n\", val_df['code'].value_counts())\n",
    "print(\"Test class distribution:\\n\", test_df['code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef573cf-7e7b-4af7-8eb9-b998403b5716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21eaab-8a6f-4361-9356-9fbbd2ff3ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff13cc3-7121-4e88-8392-f25c83a7f9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca50c71-761b-48d5-95ca-e23731769ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join('opp/data/dc_0623_psscene8b_final_norm.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, mask=True, cache=False).squeeze()\n",
    "n_bands = stack_da.values.shape[:1][0] # get a list of band names\n",
    "\n",
    "print(\n",
    "    f\"shape: {stack_da.rio.shape}\\n\"\n",
    "    f\"bands: {n_bands}\\n\"\n",
    "    f\"resolution: {stack_da.rio.resolution()}\\n\"\n",
    "    f\"bounds: {stack_da.rio.bounds()}\\n\"\n",
    "    f\"sum: {stack_da.sum().item()}\\n\"\n",
    "    f\"CRS: {stack_da.rio.crs}\\n\"\n",
    "    f\"NoData: {stack_da.rio.nodata}\\n\"\n",
    "    f\"Array: {stack_da}\"\n",
    ")\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfdcfd-1319-445a-92c9-faf298c8d4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055ca12-c480-447d-b01d-8efc157e579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params from tuning\n",
    "params = {'batch_size': 128, 'learning_rate': 0.001, 'window_size': 144}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d7160-f87e-4c9e-903b-44c966444fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d179c-0bd8-4ca3-9de8-a679e9537406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data as a Dataset\n",
    "# Uses the opimum window size calculated earlier\n",
    "\n",
    "imdir = stack_da_fp\n",
    "\n",
    "bs = params['batch_size']\n",
    "window_size = params['window_size']\n",
    "\n",
    "# Create the training samples\n",
    "train_ds = RoofImageDataset_Planet(\n",
    "    train_df[['geometry', 'code']], imdir, n_bands=n_bands, img_dim=window_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, pin_memory=True) # for GPU\n",
    "\n",
    "# Create the validation samples\n",
    "val_ds = RoofImageDataset_Planet(\n",
    "    val_df[['geometry', 'code']], imdir, n_bands=n_bands, img_dim=window_size)\n",
    "val_loader = DataLoader(val_ds, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Create the test samples\n",
    "test_ds = RoofImageDataset_Planet(\n",
    "    test_df[['geometry', 'code']], imdir, n_bands=n_bands, img_dim=window_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(\"Training and validation data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d0cb6-dcd0-424a-a68e-96abf0819354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Resnet-18 model\n",
    "\n",
    "# Define whether to leverage cpu or gpu (for my local machine it is only cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # get device for gpu or cpu\n",
    "print(f'Using {device} for model dev ...')\n",
    "\n",
    "# Grab the number of classes\n",
    "n_classes = ref_bal.class_code.unique().shape[0]\n",
    "print(f'There are {n_classes} roof type classes.')\n",
    "\n",
    "# Define the Resnet-18 model (in_channels = number of bands in the image)\n",
    "model = resnet18(n_classes, in_channels=n_bands, pretrained=False)\n",
    "\n",
    "# Make model parallel and on GPU\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    #ps_model = nn.DataParallel(ps_model)\n",
    "    model = nn.DataParallel(model)\n",
    "    print('Made cpu parallel')\n",
    "\n",
    "# optimizer, learning rate scheduler, loss criterion\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) \n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33d04c-8d55-4079-9ad3-c1cc4e10df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in each class\n",
    "val_counts = list(train_df['code'].value_counts())\n",
    "print(val_counts)\n",
    "\n",
    "total_samples = sum(val_counts)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = [total_samples / count for count in val_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the calculated class weights for verification\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Updated loss function with weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).cuda().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9c1a4-3107-4970-816e-c9a122ddeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350d162-f79d-48fd-a362-34c4982aefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print_freq = 100\n",
    "\n",
    "losses = []\n",
    "epoch_loss = []\n",
    "val_losses = []\n",
    "\n",
    "start_epoch = 0\n",
    "num_epochs = 30\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    t00 = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch}. Current learning rate: {current_lr}\")\n",
    "    \n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        # extract samples\n",
    "        image, target = batch['image'].to(device), batch['code'].to(device)\n",
    "                    \n",
    "        output = model(image.float()) \n",
    "        loss = criterion(output, target.long())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % print_freq == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, idx * len(image), len(train_loader.dataset), 100. * idx / len(train_loader), loss.item()))\n",
    "            #writer.add_scalar('train/loss', loss.item(), len(data_loader) * epoch + idx)\n",
    "            print(f'train/loss {loss.item()} {len(train_loader) * epoch + idx}')\n",
    "            losses.append((idx, loss.item()))\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        del image, target, batch, output, loss\n",
    "        gc.collect()\n",
    "                \n",
    "    # average loss for the epoch\n",
    "    epoch_loss.append(np.array(losses)[:,1].mean())\n",
    "    \n",
    "    # validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "                \n",
    "            # extract samples\n",
    "            image, target = batch['image'].to(device, non_blocking=True), batch['code'].to(device, non_blocking=True)\n",
    "\n",
    "            output = model(image)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "             # Clear variables to free up memory\n",
    "            del image, target, batch, output, pred\n",
    "            gc.collect()\n",
    "            \n",
    "    val_loss /= len(val_loader.dataset)/val_loader.batch_size\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "\n",
    "    # Adjust the learning rate based on the validation loss\n",
    "    lr_scheduler.step(val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch} completed. Current learning rate: {current_lr}\")\n",
    "    \n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"Total elapsed time for epoch {epoch}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t2:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d0fc8-11b3-4ae8-9771-2ce256d4dc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061561-38d0-428b-80f2-8a07c8fc7f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5366e-7979-4602-bde3-05c216c3d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model loss function\n",
    "plt.plot(np.array(losses)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270f143-d978-48dd-b467-d05bd7cb0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch training loss\n",
    "plt.plot(epoch_loss, label='epoch training loss')\n",
    "plt.plot(val_losses, label='epoch validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242b8aa-59fd-4eef-98e4-762e40b4740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134768f5-2fa4-4f3e-ba77-ada188abbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "save_res = True\n",
    "\n",
    "if save_res:\n",
    "    # directory\n",
    "    save_dir = 'opp/results/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # save the model\n",
    "    step = len(losses)\n",
    "    model_path = 'dc_planet_ms_ep{}_step{}_b{}.pt'.format(num_epochs, step, bs)\n",
    "    model_path = os.path.join(save_dir, model_path)\n",
    "    save = lambda ep: torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "        }, str(model_path))\n",
    "\n",
    "    save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73166d-02a0-4a12-a988-9149467ff2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(save_dir, 'losses_ep{}_step{}_b{}.txt'.format(num_epochs, step, bs)), np.array(losses))\n",
    "np.savetxt(os.path.join(save_dir, 'epoch_loss_ep{}_step{}_b{}.txt'.format(num_epochs, step, bs)), np.array(epoch_loss))\n",
    "np.savetxt(os.path.join(save_dir, 'val_losses_ep{}_step{}_b{}.txt'.format(num_epochs, step, bs)), np.array(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554d2e0-c1e1-4a85-b09e-990a82f7df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def get_prediction(m, s):\n",
    "    s = s.to(device)  # Ensure the sample is moved to the correct device\n",
    "    res = m(s)\n",
    "    return res\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "for idx, sample in enumerate(test_ds):\n",
    "    try:\n",
    "        if sample['code'] != 255:\n",
    "            # Append true labels\n",
    "            true_labels.append(sample['code'].item())  # Convert tensor to Python scalar\n",
    "                        \n",
    "            # Get prediction and append\n",
    "            pred = get_prediction(model, sample['image'][None, ...].float().to(device))  # Ensure tensor is float and on the correct device\n",
    "            pred_labels.append(pred.argmax().item())  # Convert tensor to Python scalar\n",
    "            \n",
    "        else:\n",
    "            print(f'sample {idx} of {len(test_ds)} had class label 255, skipping...')\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            time.sleep(0.3)\n",
    "            gc.collect()\n",
    "            print(f'Processed {idx} samples...')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error at sample {idx}: {e}, continuing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dba4d1-565e-4e0a-ad67-0eaa092ff625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'class_code' is of categorical dtype\n",
    "test_df['code'] = test_df['code'].astype('category')\n",
    "\n",
    "# Create the dictionary mapping\n",
    "cat_dict = dict(enumerate(test_df['code'].cat.categories))\n",
    "print(cat_dict)\n",
    "\n",
    "# Filter the class codes present in the test set\n",
    "class_codes_numbers = [k for k in cat_dict.keys() if cat_dict[k] in test_df['code'].unique().tolist()]\n",
    "\n",
    "cor_labels = [cat_dict[c] for c in class_codes_numbers]\n",
    "print(cor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb19cec-362a-4f0c-85c1-510cd791d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bbb05-a44c-46c5-8e19-c3772b90bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad) # number of trainable model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc2c6-16d6-470a-aa54-ba97d9812f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "cr_df = pd.DataFrame(classification_report(true_labels, pred_labels, target_names=cor_labels, output_dict=True)).transpose()\n",
    "print(cr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c694f93-77ad-4f62-9376-2187a428f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cor_labels:\n",
    "    print(test_df.loc[test_df['code'] == c].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a222edf-19dc-4873-9aa2-c58f41497d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_names = cor_labels # get these smartly somehow... categories got a bit mixed up\n",
    "class_labels = class_codes_numbers\n",
    "cm = confusion_matrix(true_labels, pred_labels, labels=cor_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cor_labels)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
