{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6dba8-e355-492b-b096-59ff723e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet-18 for classifying roof materials from PlanetScope SuperDove imagery\n",
    "Case study in Washington, D.C. \n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "from torchsat.models.classification import resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "from shapely.geometry import box\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion() # interactive\n",
    "\n",
    "# Projection information\n",
    "wgs = from_epsg(4326)\n",
    "proj = from_epsg(32618)\n",
    "print(f'Projected CRS: {proj}')\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/earth-lab/opp-rooftop-mapping'\n",
    "\n",
    "print(\"Successfully imported all packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3bf8-31c5-456f-a965-3c7bc461ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoofImageDataset_Planet(Dataset):\n",
    "    \"\"\"Class to handle PlanetScope SuperDove imagery for Resnet-18\"\"\"\n",
    "\n",
    "    def __init__(self, gdf, img_path, n_bands, img_dim, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gdf: Geodataframe containing 'geometry' column and 'class_code' column\n",
    "            img_path: the path to the PlanetScope SuperDove composite image (single mosaic file)\n",
    "                - see 'psscene-prep.py' for spectral indices calculation\n",
    "            imgdim (int): Image dimension for CNN implementation\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "\n",
    "        Returns image chunks with class labels\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise ValueError(f'Image does not exists: {img_path}')\n",
    "\n",
    "        self.geometries = [p.centroid for p in gdf.geometry.values] # gather centroid geoms\n",
    "        self.img_path = img_path # path to image data\n",
    "        self.img_dim = img_dim # resnet window dimension, defaults to 64\n",
    "        self.n_bands = n_bands # number of bands in the input image\n",
    "        self.Y = gdf.code.values # class codes (numeric)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.geometries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the geometry of the idx (centroid)\n",
    "        geom = self.geometries[idx]\n",
    "\n",
    "        try:\n",
    "            sample = self.sample_image(geom)  # run the sampling function\n",
    "        \n",
    "            cc = self.Y[idx]  # get the class codes\n",
    "            if type(cc) != int:\n",
    "                cc = cc.astype('uint8') # make sure the cc is an integer\n",
    "            \n",
    "            # Ensure the sample has the correct dimensions\n",
    "            assert sample.shape == (self.n_bands, self.img_dim, self.img_dim), f'Invalid sample shape: {sample.shape}'\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "            print(f\"Skipping invalid sample at index: {idx}\")\n",
    "            sample = torch.from_numpy(np.zeros((self.n_bands, int(self.img_dim), int(self.img_dim))))\n",
    "            cc = 255 # highest int8 number to be flagged\n",
    "         \n",
    "        # Convert the sample array to a Torch object\n",
    "        sample = torch.from_numpy(sample)\n",
    "\n",
    "        # Return the sample and the label as torch objects\n",
    "        return {'image': sample.type(torch.FloatTensor),\n",
    "                'code': torch.tensor(cc).type(torch.LongTensor)}\n",
    "\n",
    "    \n",
    "    def sample_image(self, geom):\n",
    "        \"\"\" Sample the image at each geometry for the specified image chunk size (window) \"\"\"\n",
    " \n",
    "        N = self.img_dim # window size to be used for cropping\n",
    "            \n",
    "        # Use the windows.from_bounds() method to return the window\n",
    "        # Returns image chunks from training data locations\n",
    "        with rio.open(self.img_path) as src:\n",
    "            py, px = src.index(geom.x, geom.y)\n",
    "            window = rio.windows.Window(px - N // 2, py - N // 2, N, N)\n",
    "            # print(window)\n",
    "            \n",
    "            # Read the data in the window\n",
    "            # clip is a nbands * N * N numpy array\n",
    "            clip = src.read(window=window, indexes=list(range(1, self.n_bands + 1)))\n",
    "\n",
    "            del py, px, window # clean up\n",
    "\n",
    "        # Convert the image chunk to a numpy array\n",
    "        clip_arr = np.array(clip)\n",
    "\n",
    "        # Check if the image chunk has valid data\n",
    "        if clip_arr.sum() > 0:\n",
    "            # Mask invalid values in each band independently\n",
    "            ans = np.ma.masked_equal(clip_arr, 0).filled(0)\n",
    "        else:\n",
    "            ans = clip_arr\n",
    "        \n",
    "        del clip, clip_arr # clean up\n",
    "        return ans\n",
    "\n",
    "\n",
    "def make_good_batch(batch):\n",
    "    \"\"\"\n",
    "    Removes bad samples if image dimensions do not match.\n",
    "    Args:\n",
    "        - batch: list of dictionaries, each containing 'image' tensor and 'code' tensor\n",
    "    returns: list of dictionaries same as input with samples having non-matching image dims removed\n",
    "    \"\"\"\n",
    "\n",
    "    _idx = torch.where(batch['code'] != 255)[0] # good batches\n",
    "\n",
    "    new_batch = {}\n",
    "    new_batch['image'] = batch['image'][_idx]\n",
    "    new_batch['code'] = batch['code'][_idx]\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def balance_sampling(df, ratio=5, strategy='undersample'):\n",
    "    \"\"\"\n",
    "    Generate balanced sample from training data based on the defined ratio.\n",
    "    This can be done with majority undersampling or minority oversampling ('strategy' parameter)\n",
    "    Args:\n",
    "        - df: the dataframe with rows as training data\n",
    "        - ratio: the sampling ration (i.e., 5:1 for minority classes default)\n",
    "    Returns:\n",
    "        - random sample with class ratios as defined\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the class counts\n",
    "    class_counts = df['class_code'].value_counts()\n",
    "    min_class_count = class_counts.min()\n",
    "    \n",
    "    # Calculate the target count for each class based on the ratio\n",
    "    target_count = {\n",
    "        class_label: min(min_class_count * ratio, len(df[df['class_code'] == class_label])) \n",
    "                        for class_label in class_counts.index\n",
    "    }\n",
    "    \n",
    "    # Create an empty list to store balanced dataframes\n",
    "    balanced_dfs = []\n",
    "    for class_label in class_counts.index:\n",
    "        class_df = df[df['class_code'] == class_label]\n",
    "        if strategy == 'undersample':\n",
    "            # Under-sample the majority class\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=False, n_samples=target_count[class_label], random_state=42)\n",
    "        elif strategy == 'oversample':\n",
    "            # Over-sample the minority class\n",
    "            balanced_class_df = resample(\n",
    "                class_df, replace=True, n_samples=target_count[class_label], random_state=42)\n",
    "        balanced_dfs.append(balanced_class_df)\n",
    "\n",
    "    # Concatenate the results by class\n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def split_training_data(gdf, ts, vs):\n",
    "    \"\"\" \n",
    "    Splits dataframe into train, test, and validation samples with the defined ratios \n",
    "    Args:\n",
    "        - gdf: training samples (geo data frame)\n",
    "        - ts: test size #\n",
    "        - vs: validation size #\n",
    "    Returns:\n",
    "        train, test, and validation dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df, test_df, val_df = [], [], []\n",
    "\n",
    "    for cl in gdf.class_code.unique():\n",
    "        \n",
    "        # subset to class\n",
    "        _gdf = gdf.loc[gdf.class_code == cl]\n",
    "        \n",
    "        # get train and test validation arrays. \n",
    "        # test array is validation array split in half.\n",
    "        _train, _valtest = train_test_split(_gdf, random_state=27, test_size=ts)\n",
    "        train_df.append(_train)\n",
    "        \n",
    "        _val, _test = train_test_split(_valtest, random_state=27, test_size=vs)\n",
    "        test_df.append(_test)\n",
    "        val_df.append(_val)\n",
    "\n",
    "    # Concatenate the samples across classes\n",
    "    all_train_df = pd.concat(train_df)\n",
    "    all_train_df = gpd.GeoDataFrame(all_train_df, crs=gdf.crs)\n",
    "    \n",
    "    all_val_df = pd.concat(val_df)\n",
    "    all_val_df = gpd.GeoDataFrame(all_val_df, crs=gdf.crs)\n",
    "    \n",
    "    all_test_df = pd.concat(test_df)\n",
    "    all_test_df = gpd.GeoDataFrame(all_test_df, crs=gdf.crs)\n",
    "\n",
    "    return all_train_df, all_val_df, all_test_df\n",
    "\n",
    "\n",
    "print(\"Class and functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0a05-5aed-4233-a6f5-c06462bae509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73acf7f-7546-4aa7-880c-489596ef92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f6824-7a2c-4d67-913e-3714fc33e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308774c2-5428-4247-adb3-836626ae7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data (footprints)\n",
    "ref_path = os.path.join('opp/data/dc_data_reference_footprints.gpkg')\n",
    "ref = gpd.read_file(ref_path)\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845358-d7eb-4350-89fd-5ae7dbbc2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the class imbalance in the reference data\n",
    "print(f\"Class counts:\\n\\n{ref.class_code.value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18afdd2-df96-4bc6-8b64-c445134f787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the shingle classes (wood shingle and shingle)\n",
    "merge = {'WS': 'WSH', 'SH': 'WSH'}\n",
    "ref['class_code'].replace(merge, inplace=True)\n",
    "ref['code'], _ = pd.factorize(ref['class_code']) # create a factorized version\n",
    "print(ref['class_code'].value_counts())  # check the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482031cc-1d68-4426-8709-6bbf1ad4eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform balanced sampling (random undersampling)\n",
    "ref_bal = balance_sampling(ref, ratio=20, strategy='undersample')\n",
    "ref_bal.code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030b0e8-3624-4946-8227-09db94678667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping class_code to code\n",
    "class_mapping = dict(zip(ref_bal['class_code'], ref_bal['code']))\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7758d-b7b0-4642-88e1-f70b94f25552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a0e89-b7d6-4fb2-8a39-ee3bfba17681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df331a-d7e5-401c-8077-f5e2053039c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/test data\n",
    "train_df, val_df, test_df = split_training_data(ref_bal, ts=0.4, vs=0.2)\n",
    "\n",
    "# Print the class distribution in training and validation sets to verify stratification\n",
    "print(\"Train class distribution:\\n\", train_df['code'].value_counts())\n",
    "print(\"Validation class distribution:\\n\", val_df['code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd122a-1ab3-425c-9390-4aca8b23f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f485d2-4279-499b-8dc9-661390f1acb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c22fa-5f03-49be-b08b-b47275bdffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca50c71-761b-48d5-95ca-e23731769ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our image data to check on the format\n",
    "stack_da_fp = os.path.join('opp/data/dc_0623_psscene8b_final_norm.tif')\n",
    "stack_da = rxr.open_rasterio(stack_da_fp, mask=True, cache=False).squeeze()\n",
    "n_bands = stack_da.values.shape[:1][0]\n",
    "print(\n",
    "    f\"shape: {stack_da.rio.shape}\\n\"\n",
    "    f\"bands: {n_bands}\\n\"\n",
    "    f\"resolution: {stack_da.rio.resolution()}\\n\"\n",
    "    f\"bounds: {stack_da.rio.bounds()}\\n\"\n",
    "    f\"sum: {stack_da.sum().item()}\\n\"\n",
    "    f\"CRS: {stack_da.rio.crs}\\n\"\n",
    "    f\"NoData: {stack_da.rio.nodata}\\n\"\n",
    "    f\"Array: {stack_da}\"\n",
    ")\n",
    "del stack_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225acc4-3619-4a3e-8b58-ec80b7c69f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464a06e-7e88-49d1-a9d7-8f55e36ee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Resnet-18 model\n",
    "\n",
    "n_bands = n_bands\n",
    "\n",
    "# Define whether to leverage cpu or gpu (for my local machine it is only cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # get device for gpu or cpu\n",
    "print(f'Using {device} for model dev ...')\n",
    "\n",
    "# Grab the number of classes\n",
    "n_classes = ref_bal.class_code.unique().shape[0]\n",
    "print(f'There are {n_classes} roof type classes.')\n",
    "\n",
    "# Define the Resnet-18 model (in_channels = number of bands in the image)\n",
    "model = resnet18(n_classes, in_channels=n_bands, pretrained=False)\n",
    "\n",
    "# Make model parallel and on GPU\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    #ps_model = nn.DataParallel(ps_model)\n",
    "    model = nn.DataParallel(model)\n",
    "    print('Made cpu parallel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33d04c-8d55-4079-9ad3-c1cc4e10df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in each class\n",
    "val_counts = list(train_df['code'].value_counts())\n",
    "print(val_counts)\n",
    "\n",
    "total_samples = sum(val_counts)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = [total_samples / count for count in val_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the calculated class weights for verification\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ffb6c-d4a0-487b-9ae1-5243892b626f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42775290-3fd6-4b70-9784-ede4e6bb2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa012653-1a9f-4e0c-acb2-b8f1b6a6025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train DataFrame indices: {val_df.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d065-53ff-4e61-9b0d-0d0760e3c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir = stack_da_fp\n",
    "\n",
    "window_size = 64\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\" Objective function for hyperparameter tuning \"\"\"\n",
    "    \"\"\"\n",
    "    Function for fine-tuning Resnet-18 model using 'optuna' Python package\n",
    "    Args:\n",
    "        - trial: Optuna trial\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Suggest hyperparameters to test\n",
    "        batch_size = trial.suggest_int('batch_size', 128, 256)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed due to: {e}\")\n",
    "        return None  # Returning None to indicate a failed trial\n",
    "        \n",
    "    # Load the train, test, and validation\n",
    "\n",
    "    # Train\n",
    "    \n",
    "    # Create the training samples\n",
    "    train_ds = RoofImageDataset_Planet(train_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Create the validation samples\n",
    "    val_ds = RoofImageDataset_Planet(val_df[['geometry', 'code']], stack_da_fp, n_bands=n_bands, img_dim=window_size)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Adjust number of epochs as needed\n",
    "        running_loss = 0.0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # Ensure a good batch\n",
    "            batch = make_good_batch(batch)\n",
    "            \n",
    "            inputs, labels = batch['image'].to(device), batch['code'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            # Ensure a good batch\n",
    "            batch = make_good_batch(batch)\n",
    "            \n",
    "            inputs, labels = batch['image'].to(device), batch['code'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"Ready !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799a6c-ad3e-46df-82e1-8df3faedc146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cad24-4dad-4fa9-ac93-e88209ba7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "t0 = time.time()\n",
    "\n",
    "study = optuna.create_study(study_name=\"Resnet-18 Hyperparameter Tuning\", direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Display the best hyperparameters and accuracy\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "t1 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t1:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9c1a4-3107-4970-816e-c9a122ddeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372c84-5493-4675-816c-c03ca020749a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061561-38d0-428b-80f2-8a07c8fc7f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a87f-dc82-4d1f-b29d-ecb16a97bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rspy",
   "language": "python",
   "name": "rspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
